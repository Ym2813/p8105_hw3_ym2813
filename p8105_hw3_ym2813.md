p8105\_hw3\_ym2813
================
Anna Ma
10/15/2021

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──

    ## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
    ## ✓ tibble  3.1.4     ✓ dplyr   1.0.7
    ## ✓ tidyr   1.1.3     ✓ stringr 1.4.0
    ## ✓ readr   2.0.1     ✓ forcats 0.5.1

    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
theme_set(theme_minimal())

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load the data set

``` r
library(p8105.datasets)
data("instacart")
```

The goal is to do some exploration of this dataset. To that end, write a
short description of the dataset, noting the size and structure of the
data, describing some key variables, and giving illstrative examples of
observations.

``` r
# Find the number of users. 
instacart %>% 
  select(user_id) %>% 
  n_distinct()
```

``` r
instacart %>% filter(order_id == 1)
```

    ## # A tibble: 8 × 15
    ##   order_id product_id add_to_cart_order reordered user_id eval_set order_number
    ##      <int>      <int>             <int>     <int>   <int> <chr>           <int>
    ## 1        1      49302                 1         1  112108 train               4
    ## 2        1      11109                 2         1  112108 train               4
    ## 3        1      10246                 3         0  112108 train               4
    ## 4        1      49683                 4         0  112108 train               4
    ## 5        1      43633                 5         1  112108 train               4
    ## 6        1      13176                 6         0  112108 train               4
    ## 7        1      47209                 7         0  112108 train               4
    ## 8        1      22035                 8         1  112108 train               4
    ## # … with 8 more variables: order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

This data set has 1384617 rows of 131209 users, and 15 columns. The rows
represents the products from the orders, and the variables includes
information about the order, user,and the product. Some of the key
variables includes order\_id, product\_id, user\_id, order\_number,
order\_dow, product\_name, aisle, and department. For example, the first
observation from the data set gives the information that this product,
identified by product\_id 49302, is in order number 1, which was placed
by the user with user id 112108. This is the first product that the user
added to the cart and it’s been ordered before, since the variable
“reordered” is 1. This is the 4th order of the user from instacart and
it was placed on the 4th day of the week at 10 o’clock. The last order
by the same user was 9 days ago. More specifically about this product,
it is the Bulgarian yogurt from the yogurt aisle, with aisle id 120 and
the dairy eggs department, with department id 16.

Then, do or answer the following (commenting on the results of each):

How many aisles are there, and which aisles are the most items ordered
from?

``` r
# The number of aisles
aisles_n = instacart %>% 
  select(aisle_id) %>% 
  n_distinct()

aisles_n 
```

    ## [1] 134

``` r
# Which aisles are the most items ordered from
aisle_most = instacart %>% 
  count(aisle,name = "aisle_sum") %>% #count aisles
  filter(aisle_sum == max(aisle_sum)) %>% # find the aisles with most count
  select(aisle)

aisle_most
```

    ## # A tibble: 1 × 1
    ##   aisle           
    ##   <chr>           
    ## 1 fresh vegetables

There are 134 aisles in total, the fresh vegetables aisle is the most
ordered from.

Plot that shows the number of items ordered in each aisle, limiting this
to aisles with more than 10000 items ordered.

``` r
instacart %>% 
  count(aisle, name = "aisle_sum") %>% 
  filter(aisle_sum >= 10000) %>% 
  arrange(aisle_sum) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = forcats::fct_reorder(aisle, sort(aisle_sum))) %>% 
  ggplot(aes(x = aisle_sum, y = aisle)) +
  geom_point() +
  labs(
    title = "Number of Item Sold in each Aisle",
    x = "Number of Items Sold",
    y = "Aisle",
    caption = "Data from instacart")
```

![](p8105_hw3_ym2813_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

I organized the aisles from the least of items ordered to the most
ordered so that the plot is more readable

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
Include the number of times each item is ordered in your table.

``` r
instacart %>% 
  group_by(aisle) %>%
  filter(aisle %in% c("baking ingredients", "dog food care" ,
         "packaged vegetables fruits")) %>% 
  count(product_name, name = "n_times_ordered") %>% 
  arrange(desc(n_times_ordered)) %>% 
  slice(1:3) %>% # only keep the top 3 rows for each group
  knitr::kable()
```

| aisle                      | product\_name                                 | n\_times\_ordered |
|:---------------------------|:----------------------------------------------|------------------:|
| baking ingredients         | Light Brown Sugar                             |               499 |
| baking ingredients         | Pure Baking Soda                              |               387 |
| baking ingredients         | Cane Sugar                                    |               336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |                30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |                28 |
| dog food care              | Small Dog Biscuits                            |                26 |
| packaged vegetables fruits | Organic Baby Spinach                          |              9784 |
| packaged vegetables fruits | Organic Raspberries                           |              5546 |
| packaged vegetables fruits | Organic Blueberries                           |              4966 |

From this table, we can see that the top three most popular item in the
baking ingredients aisle is light brown sugar, ordered 499 times, pure
baking soda, ordered 387 time, and cane sugar, ordered 336 times.
Similarly, in the dog food care aisle, the top three most popular
product is snack sticks chicken and rice recipe dog treats, ordered 30
times, organix chicken and brown rice recipe, ordered 28 times, and
small dog biscuits, ordered 26 times. Lastly, the top three most popular
product in the packaged vegetables fruits asile is organic baby spinach,
ordered 9784 times, organic raspberries, ordered 5546 times, and organic
blueberries, ordered 4966 times.

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(
    mean_hour = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>% 
  rename("Sunday" = "0", 
         "Monday" = "1", 
         "Tuesday" = "2", 
         "Wednesday" = "3",
         "Thursday" = "4",
         "Friday" = "5",
         "Saturday" = "6") %>% 
  knitr::kable()
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the `.groups` argument.

| product\_name    |   Sunday |   Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday |
|:-----------------|---------:|---------:|---------:|----------:|---------:|---------:|---------:|
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 |  15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 |  14.25000 | 11.55172 | 12.78431 | 11.93750 |

This is the table showing the mean hour of the day at which Pink Lady
Apples and Coffee Ice Cream are ordered on each day of the week. As we
can see, the mean hour of the day that people order Pink Lady Apples and
Coffee Ice Cream is in the range of 11 o’clock to 14 o’clock. The mean
varies very slightly in each day of the week.

## Problem 2

Load the data

``` r
data("brfss_smart2010")
```

# format the data to use appropriate variable names;

Focusing on the “Overall Health” topic, including response ordered from
“Poor” to “Excellent”

``` r
brfss_df = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, location = locationdesc) %>% 
  filter(topic == "Overall Health", 
         response %in% c("Excellent","Very good","Good","Fair","Poor")) %>% 
  mutate(
    response = factor(response),
    response = forcats::fct_relevel(response, c("Poor","Fair","Good","Very good","Excellent")))
```

Using this dataset, do or answer the following (commenting on the
results of each):

In 2002, which states were observed at 7 or more locations? What about
in 2010?

``` r
brfss_df %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarise(n_location_obs = n_distinct(location)) %>% 
  filter(n_location_obs >= 7)
```

    ## # A tibble: 6 × 2
    ##   state n_location_obs
    ##   <chr>          <int>
    ## 1 CT                 7
    ## 2 FL                 7
    ## 3 MA                 8
    ## 4 NC                 7
    ## 5 NJ                 8
    ## 6 PA                10

``` r
brfss_df %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarise(n_location_obs = n_distinct(location)) %>% 
  filter(n_location_obs >= 7)
```

    ## # A tibble: 14 × 2
    ##    state n_location_obs
    ##    <chr>          <int>
    ##  1 CA                12
    ##  2 CO                 7
    ##  3 FL                41
    ##  4 MA                 9
    ##  5 MD                12
    ##  6 NC                12
    ##  7 NE                10
    ##  8 NJ                19
    ##  9 NY                 9
    ## 10 OH                 8
    ## 11 PA                 7
    ## 12 SC                 7
    ## 13 TX                16
    ## 14 WA                10

In 2002, there are 6 states that were observed at 7 or more locations.
The names of the states are CT,FL,MA, NC, NJ, and PA. In 2010, there are
14 states that were observed at 7 or more locations. The names of those
states are CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA.

Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data\_value
across locations within a state.

``` r
mean_excellent_df = brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(year,state) %>% 
  mutate(
    mean_data_value = mean(data_value, na.rm = TRUE)
  ) %>% 
  select(year, state, mean_data_value) %>% 
  distinct()
```

Make a “spaghetti” plot of this average value over time within a state
(that is, make a plot showing a line for each state across years – the
geom\_line geometry and group aesthetic will help).

``` r
mean_excellent_df %>% 
  ggplot(aes(x = year, y = mean_data_value,color = state)) + 
  geom_line(aes(group = state)) 
```

![](p8105_hw3_ym2813_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

``` r
brfss_df %>% 
  filter(state == "NY", year == 2006 | year == 2010) %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .5) + 
  facet_grid(. ~ year) +
  theme(legend.position = "bottom")
```

![](p8105_hw3_ym2813_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->

## Problem 3

Load, tidy, and otherwise wrangle the data

\#encode data with reasonable variable classes??

``` r
accel_df = read_csv("data_hw3/accel_data.csv") %>% 
  pivot_longer(
    "activity.1":"activity.1440",
    names_to = "minute_of_day",
    names_prefix = "activity.",
    values_to = "activity") %>%
  mutate(
    day_type = if_else(day %in% c("Saturday","Sunday"), "weekend","weekday"),
    week = as.integer(week),
    day_id = as.integer(day_id),
    minute_of_day = as.integer(minute_of_day)
          ) %>% 
  select(week,day_id,day, day_type,minute_of_day,activity)
```

    ## Rows: 35 Columns: 1443

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

This data set have 6 variables, including the week, day, day\_id, minute
of the day, day\_type, and activity value. The set has 50400
observations. Each observation is about the activity counts for a
specific minute from a 24-hour day and the information about that day,
including the week, day\_id, and whether it is a weekday or weekend.

Aggregate accross minutes to create a total activity variable for each
day, and create a table showing these totals. \# Are any trends
apparent?

``` r
#accel_df %>% 
#  group_by(day_id) %>% 
#  summarize(total_activity = sum(activity)) %>% 
#  knitr::kable()

accel_df %>% 
  group_by(day_id) %>% 
mutate(
    total_activity = sum(activity)
  ) %>% 
  select(day_id, day, day_type, total_activity) %>% 
  distinct() %>% 
  knitr::kable()
```

| day\_id | day       | day\_type | total\_activity |
|--------:|:----------|:----------|----------------:|
|       1 | Friday    | weekday   |       480542.62 |
|       2 | Monday    | weekday   |        78828.07 |
|       3 | Saturday  | weekend   |       376254.00 |
|       4 | Sunday    | weekend   |       631105.00 |
|       5 | Thursday  | weekday   |       355923.64 |
|       6 | Tuesday   | weekday   |       307094.24 |
|       7 | Wednesday | weekday   |       340115.01 |
|       8 | Friday    | weekday   |       568839.00 |
|       9 | Monday    | weekday   |       295431.00 |
|      10 | Saturday  | weekend   |       607175.00 |
|      11 | Sunday    | weekend   |       422018.00 |
|      12 | Thursday  | weekday   |       474048.00 |
|      13 | Tuesday   | weekday   |       423245.00 |
|      14 | Wednesday | weekday   |       440962.00 |
|      15 | Friday    | weekday   |       467420.00 |
|      16 | Monday    | weekday   |       685910.00 |
|      17 | Saturday  | weekend   |       382928.00 |
|      18 | Sunday    | weekend   |       467052.00 |
|      19 | Thursday  | weekday   |       371230.00 |
|      20 | Tuesday   | weekday   |       381507.00 |
|      21 | Wednesday | weekday   |       468869.00 |
|      22 | Friday    | weekday   |       154049.00 |
|      23 | Monday    | weekday   |       409450.00 |
|      24 | Saturday  | weekend   |         1440.00 |
|      25 | Sunday    | weekend   |       260617.00 |
|      26 | Thursday  | weekday   |       340291.00 |
|      27 | Tuesday   | weekday   |       319568.00 |
|      28 | Wednesday | weekday   |       434460.00 |
|      29 | Friday    | weekday   |       620860.00 |
|      30 | Monday    | weekday   |       389080.00 |
|      31 | Saturday  | weekend   |         1440.00 |
|      32 | Sunday    | weekend   |       138421.00 |
|      33 | Thursday  | weekday   |       549658.00 |
|      34 | Tuesday   | weekday   |       367824.00 |
|      35 | Wednesday | weekday   |       445366.00 |

There’s no apparent trends

Accelerometer data allows the inspection activity over the course of the
day. Make a single-panel plot that

shows the 24-hour activity time courses for each day and use color to
indicate day of the week. Describe in words any patterns or conclusions
you can make based on this graph

``` r
total_activity_df = accel_df %>% 
  group_by(day_id) %>% 
mutate(
    total_activity = sum(activity)
  )

total_activity_df %>% 
  group_by(week) %>% 
  ggplot(aes(x = day_id, y = total_activity, color = day)) + 
  geom_point() +
  geom_line()
```

![](p8105_hw3_ym2813_files/figure-gfm/unnamed-chunk-17-1.png)<!-- -->

From the plot we can see that the total activity time for each day
differs significantly from one day to another. Moreover, even for the
same day of the week, the activity time courses are very different week
from week. This is abnormal, suggesting that there might be some mistake
in the original data, especially the recording in Saturday, where the
participants had as high as 607175 counts in week two and as low as 1440
in week 4 and 5. Therefore, we might have to go back to the original
data and corect the data before further analysis.
